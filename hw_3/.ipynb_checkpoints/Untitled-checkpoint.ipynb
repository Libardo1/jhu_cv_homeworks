{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv, cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import skimage\n",
    "import skimage.io as io\n",
    "\n",
    "sift = cv2.SIFT()\n",
    "\n",
    "def compute_images_sift_features(images):\n",
    "    \n",
    "    return map(lambda image: sift.detectAndCompute(image, None)[1], images)\n",
    "\n",
    "def train_classifier(images, number_of_clusters=1000):\n",
    "\n",
    "    img_number = len(images)\n",
    "\n",
    "    # Detect and compute Sift descriptor features in each image\n",
    "    features = compute_images_sift_features(images)\n",
    "\n",
    "    # Get the index of where for each feature the respective training image label\n",
    "    # is provided. This is done to compute histogram descriptors of each training sample\n",
    "    training_set_indexes = np.asarray([])\n",
    "\n",
    "    for ind, feature in enumerate(features):\n",
    "\n",
    "        amount_of_elements = feature.shape[0]\n",
    "        indexes_to_append = np.repeat(ind, amount_of_elements)\n",
    "        training_set_indexes = np.append(training_set_indexes, indexes_to_append)\n",
    "\n",
    "    # Put all the descriptors in one array to find clusters (bag of words)\n",
    "    training_set = np.vstack(features)\n",
    "\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
    "\n",
    "    _, labels, centers = cv2.kmeans(training_set, number_of_clusters, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n",
    "\n",
    "    # Make labels array as one dimensional list\n",
    "    labels = np.concatenate(labels)\n",
    "\n",
    "    image_descriptors = np.zeros((img_number, number_of_clusters))\n",
    "\n",
    "    for current_img_number in range(img_number):\n",
    "\n",
    "        # Get the words of the current image\n",
    "        current_img_words = labels[training_set_indexes == current_img_number]\n",
    "\n",
    "        current_img_words_number = len(current_img_words)\n",
    "\n",
    "        current_histogram = np.bincount(current_img_words, minlength=number_of_clusters).astype(np.float32)\n",
    "\n",
    "        # Normalize histogram of the current word\n",
    "        current_histogram = current_histogram / current_img_words_number\n",
    "\n",
    "        image_descriptors[current_img_number, :] = current_histogram\n",
    "\n",
    "    return centers, image_descriptors\n",
    "\n",
    "def get_images_filenames(images_folder, image_categories_folders,\n",
    "                               amount_of_images_to_take=8, fetch_from_beginnig=True):\n",
    "\n",
    "    images_filenames_list = []\n",
    "    \n",
    "    labels = np.repeat(np.arange(amount_of_first_images_to_take), 2)\n",
    "    \n",
    "    for current_category_folder in image_categories_folders:\n",
    "\n",
    "        # Get the full path to current category folder\n",
    "        current_category_folder_full_path = os.path.join(images_folder, current_category_folder)\n",
    "\n",
    "        # Get all the files in current category directory folder\n",
    "        current_category_filenames = os.listdir(current_category_folder_full_path)\n",
    "\n",
    "        # Sort all the filename in lexigraphical order. This is to get the filenames\n",
    "        # sorted like 01.jpg, 02.jpg, 03.jpg and so on.\n",
    "        current_category_filenames.sort()\n",
    "\n",
    "        # Take the images from the beginning or from the end.\n",
    "        if fetch_from_beginnig:\n",
    "            images_filenames_to_add = current_category_filenames[:amount_of_images_to_take]\n",
    "        else:\n",
    "            images_filenames_to_add = current_category_filenames[-amount_of_images_to_take:]\n",
    "\n",
    "        images_filenames_to_add = map(lambda x: os.path.join(current_category_folder_full_path, x), images_filenames_to_add)\n",
    "\n",
    "        images_filenames_list.extend(images_filenames_to_add)\n",
    "\n",
    "    return images_filenames_list, labels\n",
    "\n",
    "def closest_cluster(feature_vector, clusters):\n",
    "    \n",
    "    return np.argmin(((feature_vector - clusters)**2).sum(axis=1))\n",
    "\n",
    "def compute_bag_of_words_repr(feature_vect, clusters):\n",
    "    \n",
    "    number_of_clusters = clusters.shape[0]\n",
    "    \n",
    "    img_words_number = feature_vect.shape[0]\n",
    "    \n",
    "    bag_of_words_count = np.asarray(map(lambda x: closest_cluster(x, clusters), feature_vect))\n",
    "    \n",
    "    bag_of_words_count = np.bincount(bag_of_words_count, minlength=number_of_clusters).astype(np.float32)\n",
    "    \n",
    "    bag_of_words_frequency = bag_of_words_count / img_words_number\n",
    "    \n",
    "    return bag_of_words_frequency\n",
    "\n",
    "def compute_bag_of_words_repr_batch(feature_vectors, clusters):\n",
    "    \n",
    "    result = map(lambda x: compute_bag_of_words_repr(x, clusters), feature_vectors)\n",
    "    \n",
    "    return np.vstack(result)\n",
    "\n",
    "def images_to_bag_of_words_histogram(images, clusters):\n",
    "    \n",
    "    images_features = compute_images_sift_features(images)\n",
    "    images_histograms = compute_bag_of_words_repr_batch(images_features, centers)\n",
    "    \n",
    "    return images_histograms\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images_folder = 'images'\n",
    "# image_categories_folders = ['buildings', 'cars', 'faces', 'food', 'people', 'trees']\n",
    "# amount_of_first_images_to_take = 9\n",
    "image_categories_folders = ['buildings', 'cars']\n",
    "amount_of_first_images_to_take = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['images/buildings/01.jpg', 'images/cars/01.jpg']\n",
      "['images/buildings/11.jpg', 'images/cars/11.jpg']\n"
     ]
    }
   ],
   "source": [
    "train_images_filenames = get_images_filenames(images_folder, image_categories_folders,\n",
    "                                              amount_of_first_images_to_take)\n",
    "\n",
    "test_images_filenames = get_images_filenames(images_folder, image_categories_folders,\n",
    "                                            amount_of_first_images_to_take, fetch_from_beginnig=False)\n",
    "\n",
    "print train_images_filenames\n",
    "print test_images_filenames\n",
    "\n",
    "train_images = io.imread_collection(train_images_filenames)\n",
    "test_images = io.imread_collection(test_images_filenames)\n",
    "\n",
    "cluster_centers, train_images_histograms = train_classifier(train_images)\n",
    "\n",
    "test_images_histograms = images_to_bag_of_words_histogram(test_images, cluster_centers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.repeat(np.arange(2), 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
